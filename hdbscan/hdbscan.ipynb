{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import optuna\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Analize the distribution of the continous columns*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "continuous_cols = ['age', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "\n",
    "# Create subplots for distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Distribution of Continuous Features', fontsize=16)\n",
    "\n",
    "# Plot distribution for each continuous variable\n",
    "for idx, col in enumerate(continuous_cols):\n",
    "    ax = axes[idx//2, idx%2]\n",
    "    \n",
    "    # Plot histogram and kernel density estimate\n",
    "    sns.histplot(data=df, x=col, kde=True, ax=ax)\n",
    "    \n",
    "    # Add basic statistics\n",
    "    mean_val = df[col].mean()\n",
    "    median_val = df[col].median()\n",
    "    ax.axvline(mean_val, color='red', linestyle='--', label=f'Mean: {mean_val:.2f}')\n",
    "    ax.axvline(median_val, color='green', linestyle='--', label=f'Median: {median_val:.2f}')\n",
    "    ax.legend()\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "# Print skewness and kurtosis\n",
    "print(\"\\nDistribution Statistics:\")\n",
    "for col in continuous_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"Skewness: {stats.skew(df[col]):.3f}\")\n",
    "    print(f\"Kurtosis: {stats.kurtosis(df[col]):.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Change the Scale of capital-gain and capital-loss and apply robust scaling to all the features*\n",
    "\n",
    "Considering the distribution of the continuous features (in particular capital-gain and capital-loss), log(1+x) transformation is required. This is done for reducing the extreme values while preserving the relative differences. Considering that HDBSCAN works with distance-based density estimation, extreme values can dominate distance calculations. \n",
    "\n",
    "After that, apply RobustScaler so that the features are brought to comparable scales and distances calculations become more meaningful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = df.copy()\n",
    "\n",
    "# Log1p transformation for financial features\n",
    "df_transformed['capital-gain'] = np.log1p(df_transformed['capital-gain'])\n",
    "df_transformed['capital-loss'] = np.log1p(df_transformed['capital-loss'])\n",
    "\n",
    "# Apply RobustScaler to all numerical features\n",
    "numerical_features = ['age', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "scaler = RobustScaler()\n",
    "df_transformed[numerical_features] = scaler.fit_transform(df_transformed[numerical_features])\n",
    "\n",
    "\n",
    "## Uncomment this to plot the distributions after the transformations\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "# fig.suptitle('Distribution Before and After Transformation', fontsize=16)\n",
    "\n",
    "# for idx, col in enumerate(['capital-gain', 'capital-loss']):\n",
    "#     # Original distribution\n",
    "#     sns.histplot(data=df, x=col, kde=True, ax=axes[idx, 0])\n",
    "#     axes[idx, 0].set_title(f'Original {col}')\n",
    "    \n",
    "#     # Transformed distribution\n",
    "#     sns.histplot(data=df_transformed, x=col, kde=True, ax=axes[idx, 1])\n",
    "#     axes[idx, 1].set_title(f'Transformed {col}')\n",
    "\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the labels and drop them for clustering\n",
    "X = df_transformed.drop('income', axis=1)  # Features (all columns except 'income')\n",
    "true_labels = df['income'].values  # True labels from 'income' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding the best set of hyperparameters for HDBSCAN**\n",
    "\n",
    "Need to prioritize cluster purity, in particular Pattern Discovery (Silhouette score). Using Optuna for finding the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X, true_labels):\n",
    "    \"\"\"\n",
    "    Optuna objective function for optimizing HDBSCAN parameters\n",
    "    \"\"\"\n",
    "    # Define the parameter search space\n",
    "    min_cluster_size = trial.suggest_int('min_cluster_size', 50, 500)\n",
    "    min_samples = trial.suggest_int('min_samples', max(5, min_cluster_size//10), min_cluster_size)\n",
    "    cluster_selection_epsilon = trial.suggest_float('cluster_selection_epsilon', 0.0, 1.0)\n",
    "    \n",
    "    # Initialize HDBSCAN with trial parameters\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=min_samples,\n",
    "        cluster_selection_epsilon=cluster_selection_epsilon,\n",
    "        metric='euclidean',\n",
    "        cluster_selection_method='eom',\n",
    "        prediction_data=True\n",
    "    )\n",
    "    \n",
    "    # Fit and predict\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "    \n",
    "    n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "    \n",
    "    # If only one cluster or all points are noise, return worst possible score\n",
    "    if n_clusters <= 1:\n",
    "        return -float('inf')\n",
    "    \n",
    "    # Calculate silhouette score for non-noise points\n",
    "    non_noise_mask = cluster_labels != -1\n",
    "    if non_noise_mask.sum() > 1:\n",
    "        sil_score = silhouette_score(X[non_noise_mask], cluster_labels[non_noise_mask])\n",
    "    else:\n",
    "        sil_score = -float('inf')\n",
    "    \n",
    "    # Calculate other metrics\n",
    "    ari_score = adjusted_rand_score(true_labels, cluster_labels)\n",
    "    nmi_score = normalized_mutual_info_score(true_labels, cluster_labels)\n",
    "    noise_ratio = (cluster_labels == -1).sum() / len(cluster_labels)\n",
    "    \n",
    "    # Objective score (weights)\n",
    "    objective_score = (\n",
    "        0.4 * sil_score +      # Weight for cluster quality\n",
    "        0.3 * ari_score +      # Weight for agreement with true labels\n",
    "        0.2 * nmi_score -      # Weight for information shared with true labels\n",
    "        0.1 * noise_ratio      # Small penalty for excessive noise\n",
    "    )\n",
    "    \n",
    "    # Store additional metrics for analysis\n",
    "    trial.set_user_attr('n_clusters', n_clusters)\n",
    "    trial.set_user_attr('silhouette', sil_score)\n",
    "    trial.set_user_attr('ari', ari_score)\n",
    "    trial.set_user_attr('nmi', nmi_score)\n",
    "    trial.set_user_attr('noise_ratio', noise_ratio)\n",
    "    \n",
    "    return objective_score\n",
    "\n",
    "def optimize_hdbscan(X, true_labels, n_trials=100):\n",
    "    \"\"\"\n",
    "    Run Optuna optimization for HDBSCAN parameters\n",
    "    \"\"\"\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(trial, X, true_labels), n_trials=n_trials)\n",
    "    \n",
    "    # Get best parameters\n",
    "    best_params = study.best_trial.params\n",
    "    \n",
    "    # Print optimization results\n",
    "    print(\"\\nOptimization Results:\")\n",
    "    print(\"Best parameters:\", best_params)\n",
    "    print(\"Best trial metrics:\")\n",
    "    print(f\"Number of clusters: {study.best_trial.user_attrs['n_clusters']}\")\n",
    "    print(f\"Silhouette score: {study.best_trial.user_attrs['silhouette']:.3f}\")\n",
    "    print(f\"Adjusted Rand Index: {study.best_trial.user_attrs['ari']:.3f}\")\n",
    "    print(f\"NMI score: {study.best_trial.user_attrs['nmi']:.3f}\")\n",
    "    print(f\"Noise ratio: {study.best_trial.user_attrs['noise_ratio']:.3f}\")\n",
    "    \n",
    "    return best_params, study\n",
    "\n",
    "# Usage:\n",
    "best_params, study = optimize_hdbscan(X, true_labels, n_trials=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model training**\n",
    "\n",
    "Once found the best set of hyperparameters, create the model using them. Then evaluate the result obtained by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=220,\n",
    "    min_samples=117,\n",
    "    metric='euclidean',\n",
    "    cluster_selection_method='eom',\n",
    "    prediction_data=True,\n",
    "    cluster_selection_epsilon=0.28479667859306007\n",
    ")\n",
    "\n",
    "cluster_labels = clusterer.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "\n",
    "non_noise_mask = cluster_labels != -1\n",
    "sil_score = silhouette_score(X[non_noise_mask], cluster_labels[non_noise_mask])\n",
    "\n",
    "\n",
    "ari_score = adjusted_rand_score(true_labels, cluster_labels)\n",
    "nmi_score = normalized_mutual_info_score(true_labels, cluster_labels)\n",
    "noise_ratio = (cluster_labels == -1).sum() / len(cluster_labels)\n",
    "\n",
    "print(f\"Number of clusters: {n_clusters}\")\n",
    "print(f\"Silhouette score: {sil_score}\")\n",
    "print(f\"Adjusted Rand Index: {ari_score}\")\n",
    "print(f\"NMI score: {nmi_score}\")\n",
    "print(f\"Noise ratio: {noise_ratio}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze intra-cluster distributions and characteristics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cluster_distributions(df_transformed, cluster_labels):\n",
    "    \"\"\"\n",
    "    Analyze the distribution of features within each cluster using transformed data\n",
    "    Including PCA components from workclass and occupation\n",
    "    \"\"\"\n",
    "    # Add cluster labels to the dataframe\n",
    "    df_with_clusters = df_transformed.copy()\n",
    "    df_with_clusters['Cluster'] = cluster_labels\n",
    "    \n",
    "    # Define feature groups\n",
    "    base_numerical_features = ['age', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "    pca_features = [f'pca_component_{i+1}' for i in range(10)]  # PCA components\n",
    "    all_numerical_features = base_numerical_features + pca_features\n",
    "    \n",
    "    categorical_features = [col for col in df_transformed.columns \n",
    "                          if col not in all_numerical_features + ['Cluster', 'income']]\n",
    "    \n",
    "    # Create summary statistics for each cluster\n",
    "    summary_stats = []\n",
    "    for cluster in sorted(set(cluster_labels)):\n",
    "        cluster_name = \"Noise\" if cluster == -1 else f\"Cluster {cluster}\"\n",
    "        cluster_data = df_with_clusters[df_with_clusters['Cluster'] == cluster]\n",
    "        \n",
    "        cluster_stats = {}\n",
    "        \n",
    "        # Analyze base numerical features\n",
    "        for feature in base_numerical_features:\n",
    "            stats_dict = {\n",
    "                'mean': cluster_data[feature].mean(),\n",
    "                'median': cluster_data[feature].median(),\n",
    "                'std': cluster_data[feature].std(),\n",
    "                'min': cluster_data[feature].min(),\n",
    "                'max': cluster_data[feature].max(),\n",
    "                'q1': cluster_data[feature].quantile(0.25),\n",
    "                'q3': cluster_data[feature].quantile(0.75)\n",
    "            }\n",
    "            cluster_stats[feature] = stats_dict\n",
    "        \n",
    "        # Analyze PCA components\n",
    "        for feature in pca_features:\n",
    "            stats_dict = {\n",
    "                'mean': cluster_data[feature].mean(),\n",
    "                'median': cluster_data[feature].median(),\n",
    "                'std': cluster_data[feature].std(),\n",
    "                'min': cluster_data[feature].min(),\n",
    "                'max': cluster_data[feature].max(),\n",
    "                'q1': cluster_data[feature].quantile(0.25),\n",
    "                'q3': cluster_data[feature].quantile(0.75)\n",
    "            }\n",
    "            cluster_stats[feature] = stats_dict\n",
    "            \n",
    "        # Analyze remaining categorical features\n",
    "        for feature in categorical_features:\n",
    "            value_counts = cluster_data[feature].value_counts(normalize=True).nlargest(3)\n",
    "            cluster_stats[f'top_{feature}'] = value_counts.to_dict()\n",
    "        \n",
    "        summary_stats.append({\n",
    "            'cluster': cluster_name,\n",
    "            'size': len(cluster_data),\n",
    "            'stats': cluster_stats\n",
    "        })\n",
    "    \n",
    "    return summary_stats\n",
    "\n",
    "def plot_feature_distributions(df_transformed, cluster_labels, feature):\n",
    "    \"\"\"\n",
    "    Plot the distribution of transformed features across different clusters\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for cluster in sorted(set(cluster_labels)):\n",
    "        label = 'Noise' if cluster == -1 else f'Cluster {cluster}'\n",
    "        cluster_data = df_transformed[cluster_labels == cluster][feature]\n",
    "        sns.kdeplot(data=cluster_data, label=label)\n",
    "    \n",
    "    title = f'Distribution of {feature}'\n",
    "    if feature.startswith('PC'):\n",
    "        title += ' (PCA component from workclass/occupation)'\n",
    "    else:\n",
    "        title += ' (transformed)'\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def print_cluster_profiles(summary_stats):\n",
    "    \"\"\"\n",
    "    Print detailed profiles for each cluster\n",
    "    \"\"\"\n",
    "    for cluster_info in summary_stats:\n",
    "        print(f\"\\n{'-'*50}\")\n",
    "        print(f\"{cluster_info['cluster']} (Size: {cluster_info['size']})\")\n",
    "        print(f\"{'-'*50}\")\n",
    "        \n",
    "        stats = cluster_info['stats']\n",
    "        \n",
    "        # Print base numerical features statistics\n",
    "        print(\"\\nBase Numerical Features (Transformed):\")\n",
    "        for feature in ['age', 'capital-gain', 'capital-loss', 'hours-per-week']:\n",
    "            print(f\"\\n{feature}:\")\n",
    "            feature_stats = stats[feature]\n",
    "            print(f\"  Mean: {feature_stats['mean']:.3f}\")\n",
    "            print(f\"  Median: {feature_stats['median']:.3f}\")\n",
    "            print(f\"  Std: {feature_stats['std']:.3f}\")\n",
    "            print(f\"  IQR: [{feature_stats['q1']:.3f} - {feature_stats['q3']:.3f}]\")\n",
    "            print(f\"  Range: [{feature_stats['min']:.3f} - {feature_stats['max']:.3f}]\")\n",
    "        \n",
    "        # Print PCA components statistics\n",
    "        print(\"\\nPCA Components (workclass/occupation):\")\n",
    "        for i in range(10):\n",
    "            feature = f'pca_component_{i+1}'\n",
    "            print(f\"\\n{feature}:\")\n",
    "            feature_stats = stats[feature]\n",
    "            print(f\"  Mean: {feature_stats['mean']:.3f}\")\n",
    "            print(f\"  Median: {feature_stats['median']:.3f}\")\n",
    "            print(f\"  Std: {feature_stats['std']:.3f}\")\n",
    "            print(f\"  IQR: [{feature_stats['q1']:.3f} - {feature_stats['q3']:.3f}]\")\n",
    "        \n",
    "        # Print categorical features statistics\n",
    "        if any(key.startswith('top_') for key in stats):\n",
    "            print(\"\\nCategorical Features:\")\n",
    "            for key in stats:\n",
    "                if key.startswith('top_'):\n",
    "                    feature = key.split('_', 1)[1]\n",
    "                    print(f\"\\n{feature}:\")\n",
    "                    for category, percentage in stats[key].items():\n",
    "                        print(f\"  {category}: {percentage:.1%}\")\n",
    "\n",
    "# Usage:\n",
    "summary_stats = analyze_cluster_distributions(df, cluster_labels)\n",
    "print_cluster_profiles(summary_stats)\n",
    "# \n",
    "#Plot base features\n",
    "# for feature in ['age', 'capital-gain', 'capital-loss', 'hours-per-week']:\n",
    "#     plot_feature_distributions(df, cluster_labels, feature)\n",
    "#\n",
    "# # Plot PCA components\n",
    "# for i in range(10):\n",
    "#     plot_feature_distributions(df_transformed, f'PC{i+1}_work_occ', cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze the distribution of the income variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_income_distribution(df, cluster_labels, income_labels):\n",
    "    \"\"\"\n",
    "    Analyze how income is distributed across clusters\n",
    "    \"\"\"\n",
    "    df_analysis = pd.DataFrame({\n",
    "        'Cluster': cluster_labels,\n",
    "        'Income': income_labels\n",
    "    })\n",
    "    \n",
    "    # Overall cluster composition\n",
    "    print(\"Cluster Income Distribution:\")\n",
    "    for cluster in sorted(set(cluster_labels)):\n",
    "        cluster_mask = df_analysis['Cluster'] == cluster\n",
    "        cluster_size = cluster_mask.sum()\n",
    "        \n",
    "        # Get income distribution for this cluster\n",
    "        income_dist = df_analysis[cluster_mask]['Income'].value_counts(normalize=True)\n",
    "        income_counts = df_analysis[cluster_mask]['Income'].value_counts()\n",
    "        \n",
    "        cluster_name = \"Noise\" if cluster == -1 else f\"Cluster {cluster}\"\n",
    "        print(f\"\\n{cluster_name} (Size: {cluster_size})\")\n",
    "        print(f\"<=50K: {income_counts.get(0, 0)} ({income_dist.get(0, 0)*100:.1f}%)\")\n",
    "        print(f\">50K: {income_counts.get(1, 0)} ({income_dist.get(1, 0)*100:.1f}%)\")\n",
    "        \n",
    "        # Calculate correlation strength\n",
    "        if cluster != -1:  # Skip noise points\n",
    "            correlation = np.corrcoef(cluster_mask, income_labels)[0,1]\n",
    "            print(f\"Correlation with income: {correlation:.3f}\")\n",
    "\n",
    "def plot_income_distribution(cluster_labels, income_labels):\n",
    "    \"\"\"\n",
    "    Visualize income distribution across clusters\n",
    "    \"\"\"\n",
    "    # Create stacked bar chart\n",
    "    df_plot = pd.DataFrame({\n",
    "        'Cluster': cluster_labels,\n",
    "        'Income': ['<=50K' if x == 0 else '>50K' for x in income_labels]\n",
    "    })\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    cluster_income = pd.crosstab(df_plot['Cluster'], df_plot['Income'], normalize='index')\n",
    "    cluster_income.plot(kind='bar', stacked=True)\n",
    "    plt.title('Income Distribution by Cluster')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.legend(title='Income')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_clustering_performance(cluster_labels, income_labels):\n",
    "    \"\"\"\n",
    "    Calculate and analyze clustering performance metrics\n",
    "    \"\"\"\n",
    "    # Calculate metrics\n",
    "    ari_score = adjusted_rand_score(income_labels, cluster_labels)\n",
    "    nmi_score = normalized_mutual_info_score(income_labels, cluster_labels)\n",
    "    \n",
    "    print(\"\\nClustering Performance Metrics:\")\n",
    "    print(f\"Adjusted Rand Index: {ari_score:.3f}\")\n",
    "    print(f\"Normalized Mutual Information: {nmi_score:.3f}\")\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cluster_income_matrix = confusion_matrix(income_labels, cluster_labels)\n",
    "    \n",
    "    # Analyze mismatches\n",
    "    print(\"\\nDetailed Cluster-Income Analysis:\")\n",
    "    unique_clusters = sorted(set(cluster_labels))\n",
    "    for cluster in unique_clusters:\n",
    "        cluster_mask = cluster_labels == cluster\n",
    "        total_in_cluster = cluster_mask.sum()\n",
    "        correct_predictions = (income_labels[cluster_mask] == cluster).sum()\n",
    "        \n",
    "        cluster_name = \"Noise\" if cluster == -1 else f\"Cluster {cluster}\"\n",
    "        print(f\"\\n{cluster_name}:\")\n",
    "        print(f\"Total points: {total_in_cluster}\")\n",
    "        print(f\"Match with majority class: {(correct_predictions/total_in_cluster)*100:.1f}%\")\n",
    "\n",
    "def analyze_income_patterns(df, cluster_labels, income_labels):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of income patterns in clusters\n",
    "    \"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"Income Distribution Analysis\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Basic distribution analysis\n",
    "    analyze_income_distribution(df, cluster_labels, income_labels)\n",
    "    \n",
    "    # Visualization\n",
    "    plot_income_distribution(cluster_labels, income_labels)\n",
    "    \n",
    "    # Performance metrics\n",
    "    analyze_clustering_performance(cluster_labels, income_labels)\n",
    "\n",
    "analyze_income_patterns(df_transformed, cluster_labels, true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "\n",
    "def visualize_clusters_clear_colors(df_transformed, cluster_labels, sample_size=5000, random_state=42):\n",
    "    \"\"\"\n",
    "    Create visualizations with clear color distinction between clusters\n",
    "    \"\"\"\n",
    "    # Sample the data\n",
    "    if len(df_transformed) > sample_size:\n",
    "        idx = np.random.RandomState(random_state).choice(\n",
    "            len(df_transformed), sample_size, replace=False\n",
    "        )\n",
    "        df_sample = df_transformed.iloc[idx]\n",
    "        clusters_sample = cluster_labels[idx]\n",
    "    else:\n",
    "        df_sample = df_transformed\n",
    "        clusters_sample = cluster_labels\n",
    "    \n",
    "    # Create custom color map for better visibility\n",
    "    cluster_colors = {\n",
    "        -1: 'gray',    # Noise points in gray\n",
    "        0: 'blue',     # Cluster 0 in blue\n",
    "        1: 'red'       # Cluster 1 in red\n",
    "    }\n",
    "    \n",
    "    # Convert cluster labels to colors\n",
    "    colors = [cluster_colors[label] for label in clusters_sample]\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(20, 6))\n",
    "    \n",
    "    # 1. PCA\n",
    "    print(\"Computing PCA projection...\")\n",
    "    pca = PCA(n_components=2, random_state=random_state)\n",
    "    embedding_pca = pca.fit_transform(df_sample)\n",
    "    \n",
    "    ax1 = fig.add_subplot(131)\n",
    "    for cluster in [-1, 0, 1]:\n",
    "        mask = clusters_sample == cluster\n",
    "        label = 'Noise' if cluster == -1 else f'Cluster {cluster}'\n",
    "        ax1.scatter(embedding_pca[mask, 0], embedding_pca[mask, 1], \n",
    "                   c=cluster_colors[cluster], \n",
    "                   alpha=0.6,\n",
    "                   label=label)\n",
    "    ax1.set_title(f'PCA Projection\\nExplained variance: {pca.explained_variance_ratio_.sum():.2%}')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 2. UMAP\n",
    "    print(\"Computing UMAP projection...\")\n",
    "    reducer = umap.UMAP(random_state=random_state)\n",
    "    embedding_umap = reducer.fit_transform(df_sample)\n",
    "    \n",
    "    ax2 = fig.add_subplot(132)\n",
    "    for cluster in [-1, 0, 1]:\n",
    "        mask = clusters_sample == cluster\n",
    "        label = 'Noise' if cluster == -1 else f'Cluster {cluster}'\n",
    "        ax2.scatter(embedding_umap[mask, 0], embedding_umap[mask, 1], \n",
    "                   c=cluster_colors[cluster], \n",
    "                   alpha=0.6,\n",
    "                   label=label)\n",
    "    ax2.set_title('UMAP Projection')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # 3. t-SNE\n",
    "    print(\"Computing t-SNE projection...\")\n",
    "    tsne = TSNE(n_components=2, random_state=random_state)\n",
    "    embedding_tsne = tsne.fit_transform(df_sample)\n",
    "    \n",
    "    ax3 = fig.add_subplot(133)\n",
    "    for cluster in [-1, 0, 1]:\n",
    "        mask = clusters_sample == cluster\n",
    "        label = 'Noise' if cluster == -1 else f'Cluster {cluster}'\n",
    "        ax3.scatter(embedding_tsne[mask, 0], embedding_tsne[mask, 1], \n",
    "                   c=cluster_colors[cluster], \n",
    "                   alpha=0.6,\n",
    "                   label=label)\n",
    "    ax3.set_title('t-SNE Projection')\n",
    "    ax3.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print explained variance ratios for PCA\n",
    "    print(\"\\nPCA Explained Variance Ratios:\")\n",
    "    print(f\"PC1: {pca.explained_variance_ratio_[0]:.2%}\")\n",
    "    print(f\"PC2: {pca.explained_variance_ratio_[1]:.2%}\")\n",
    "    print(f\"Total: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "    # Print cluster sizes\n",
    "    unique, counts = np.unique(clusters_sample, return_counts=True)\n",
    "    print(\"\\nCluster Sizes in Sample:\")\n",
    "    for cluster, count in zip(unique, counts):\n",
    "        label = 'Noise' if cluster == -1 else f'Cluster {cluster}'\n",
    "        print(f\"{label}: {count} points ({count/len(clusters_sample):.1%})\")\n",
    "\n",
    "# Usage:\n",
    "visualize_clusters_clear_colors(df_transformed, cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How, by changing a cost function, the demographics get affected?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_demographic_impact(df, cluster_labels, demographic_column):\n",
    "    \"\"\"\n",
    "    Analyze how clustering affects different demographic groups\n",
    "    \"\"\"\n",
    "    impact_analysis = pd.DataFrame({\n",
    "        'Cluster': cluster_labels,\n",
    "        'Demographic': df[demographic_column]\n",
    "    })\n",
    "    \n",
    "    # Distribution of demographics across clusters\n",
    "    cluster_demographics = pd.crosstab(\n",
    "        impact_analysis['Cluster'], \n",
    "        impact_analysis['Demographic'],\n",
    "        normalize='index'\n",
    "    )\n",
    "    \n",
    "    # Noise point analysis by demographic\n",
    "    noise_rates = impact_analysis[impact_analysis['Cluster'] == -1]['Demographic'].value_counts(normalize=True)\n",
    "    \n",
    "    return cluster_demographics, noise_rates\n",
    "\n",
    "analyze_demographic_impact(df, cluster_labels, 'gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export the model**\n",
    "\n",
    "Used later for visualization with Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(clusterer, 'hdbscan_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = joblib.load('hdbscan_model.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
