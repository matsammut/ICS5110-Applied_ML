{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import optuna\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score, confusion_matrix\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Analize the distribution of the continous columns*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "continuous_cols = ['age', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "\n",
    "# Create subplots for distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Distribution of Continuous Features', fontsize=16)\n",
    "\n",
    "for idx, col in enumerate(continuous_cols):\n",
    "    ax = axes[idx//2, idx%2]\n",
    "    \n",
    "    # Plot histogram and kernel density estimate\n",
    "    sns.histplot(data=df, x=col, kde=True, ax=ax)\n",
    "    \n",
    "    # Add basic statistics\n",
    "    mean_val = df[col].mean()\n",
    "    median_val = df[col].median()\n",
    "    ax.axvline(mean_val, color='red', linestyle='--', label=f'Mean: {mean_val:.2f}')\n",
    "    ax.axvline(median_val, color='green', linestyle='--', label=f'Median: {median_val:.2f}')\n",
    "    ax.legend()\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "print(\"\\nDistribution Statistics:\")\n",
    "for col in continuous_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"Skewness: {stats.skew(df[col]):.3f}\")\n",
    "    print(f\"Kurtosis: {stats.kurtosis(df[col]):.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Change the Scale of capital-gain and capital-loss and apply robust scaling to all the features*\n",
    "\n",
    "Considering the distribution of the continuous features (in particular capital-gain and capital-loss), log(1+x) transformation is required. This is done for reducing the extreme values while preserving the relative differences. Considering that HDBSCAN works with distance-based density estimation, extreme values can dominate distance calculations. \n",
    "\n",
    "After that, apply RobustScaler so that the features are brought to comparable scales and distances calculations become more meaningful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = df.copy()\n",
    "\n",
    "# Log1p transformation for financial features\n",
    "df_transformed['capital-gain'] = np.log1p(df_transformed['capital-gain'])\n",
    "df_transformed['capital-loss'] = np.log1p(df_transformed['capital-loss'])\n",
    "\n",
    "# Apply RobustScaler to all numerical features\n",
    "numerical_features = ['age', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "scaler = RobustScaler()\n",
    "df_transformed[numerical_features] = scaler.fit_transform(df_transformed[numerical_features])\n",
    "\n",
    "#joblib.dump(scaler, \"robust_scaler.pkl\")\n",
    "\n",
    "\n",
    "## Uncomment this to plot the distributions after the transformations\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "# fig.suptitle('Distribution Before and After Transformation', fontsize=16)\n",
    "\n",
    "# for idx, col in enumerate(['capital-gain', 'capital-loss']):\n",
    "#     # Original distribution\n",
    "#     sns.histplot(data=df, x=col, kde=True, ax=axes[idx, 0])\n",
    "#     axes[idx, 0].set_title(f'Original {col}')\n",
    "    \n",
    "#     # Transformed distribution\n",
    "#     sns.histplot(data=df_transformed, x=col, kde=True, ax=axes[idx, 1])\n",
    "#     axes[idx, 1].set_title(f'Transformed {col}')\n",
    "\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the labels and drop them for clustering\n",
    "X = df_transformed.drop('income', axis=1)  # Features (all columns except 'income')\n",
    "true_labels = df['income'].values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding the best set of hyperparameters for HDBSCAN**\n",
    "\n",
    "Need to prioritize cluster purity, in particular Pattern Discovery (Silhouette score). Using Optuna for finding the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X, true_labels):\n",
    "    \"\"\"\n",
    "    Optuna objective function for optimizing HDBSCAN parameters\n",
    "    \"\"\"\n",
    "    # Define the parameter search space\n",
    "    min_cluster_size = trial.suggest_int('min_cluster_size', 50, 500)\n",
    "    min_samples = trial.suggest_int('min_samples', max(5, min_cluster_size//10), min_cluster_size)\n",
    "    cluster_selection_epsilon = trial.suggest_float('cluster_selection_epsilon', 0.0, 1.0)\n",
    "    \n",
    "    # Initialize HDBSCAN with trial parameters\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=min_samples,\n",
    "        cluster_selection_epsilon=cluster_selection_epsilon,\n",
    "        metric='euclidean',\n",
    "        cluster_selection_method='eom',\n",
    "        prediction_data=True\n",
    "    )\n",
    "    \n",
    "    # Fit and predict\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "    \n",
    "    n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "    \n",
    "    # If only one cluster or all points are noise, return worst possible score\n",
    "    if n_clusters <= 1:\n",
    "        return -float('inf')\n",
    "    \n",
    "    # Calculate silhouette score for non-noise points\n",
    "    non_noise_mask = cluster_labels != -1\n",
    "    if non_noise_mask.sum() > 1:\n",
    "        sil_score = silhouette_score(X[non_noise_mask], cluster_labels[non_noise_mask])\n",
    "    else:\n",
    "        sil_score = -float('inf')\n",
    "    \n",
    "    # Calculate other metrics\n",
    "    ari_score = adjusted_rand_score(true_labels, cluster_labels)\n",
    "    nmi_score = normalized_mutual_info_score(true_labels, cluster_labels)\n",
    "    noise_ratio = (cluster_labels == -1).sum() / len(cluster_labels)\n",
    "    \n",
    "    # Objective score (weights)\n",
    "    objective_score = (\n",
    "        0.4 * sil_score +      # Weight for cluster quality (Silhouette score)\n",
    "        0.3 * ari_score +      # Weight for agreement with true labels\n",
    "        0.2 * nmi_score -      # Weight for information shared with true labels\n",
    "        0.1 * noise_ratio      # Small penalty for excessive noise\n",
    "    )\n",
    "    \n",
    "    # Store additional metrics for analysis\n",
    "    trial.set_user_attr('n_clusters', n_clusters)\n",
    "    trial.set_user_attr('silhouette', sil_score)\n",
    "    trial.set_user_attr('ari', ari_score)\n",
    "    trial.set_user_attr('nmi', nmi_score)\n",
    "    trial.set_user_attr('noise_ratio', noise_ratio)\n",
    "    \n",
    "    return objective_score\n",
    "\n",
    "def optimize_hdbscan(X, true_labels, n_trials=100):\n",
    "    \"\"\"\n",
    "    Run Optuna optimization for HDBSCAN parameters\n",
    "    \"\"\"\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(trial, X, true_labels), n_trials=n_trials)\n",
    "    \n",
    "    # Get best parameters\n",
    "    best_params = study.best_trial.params\n",
    "    \n",
    "    # Print optimization results\n",
    "    print(\"\\nOptimization Results:\")\n",
    "    print(\"Best parameters:\", best_params)\n",
    "    print(\"Best trial metrics:\")\n",
    "    print(f\"Number of clusters: {study.best_trial.user_attrs['n_clusters']}\")\n",
    "    print(f\"Silhouette score: {study.best_trial.user_attrs['silhouette']:.3f}\")\n",
    "    print(f\"Adjusted Rand Index: {study.best_trial.user_attrs['ari']:.3f}\")\n",
    "    print(f\"NMI score: {study.best_trial.user_attrs['nmi']:.3f}\")\n",
    "    print(f\"Noise ratio: {study.best_trial.user_attrs['noise_ratio']:.3f}\")\n",
    "    \n",
    "    return best_params, study\n",
    "\n",
    "# Usage:\n",
    "best_params, study = optimize_hdbscan(X, true_labels, n_trials=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model training**\n",
    "\n",
    "Once found the best set of hyperparameters, create the model using them. Then evaluate the result obtained by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HDBSCAN with optimized parameters\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=best_params['min_cluster_size'],\n",
    "    min_samples=best_params['min_samples'],\n",
    "    cluster_selection_epsilon=best_params['cluster_selection_epsilon'],\n",
    "    metric='euclidean',\n",
    "    cluster_selection_method='eom',\n",
    "    prediction_data=True\n",
    ")\n",
    "\n",
    "# Fit and predict\n",
    "cluster_labels = clusterer.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "\n",
    "non_noise_mask = cluster_labels != -1\n",
    "sil_score = silhouette_score(X[non_noise_mask], cluster_labels[non_noise_mask])\n",
    "\n",
    "\n",
    "ari_score = adjusted_rand_score(true_labels, cluster_labels)\n",
    "nmi_score = normalized_mutual_info_score(true_labels, cluster_labels)\n",
    "noise_ratio = (cluster_labels == -1).sum() / len(cluster_labels)\n",
    "\n",
    "print(f\"Number of clusters: {n_clusters}\")\n",
    "print(f\"Silhouette score: {sil_score}\")\n",
    "print(f\"Adjusted Rand Index: {ari_score}\")\n",
    "print(f\"NMI score: {nmi_score}\")\n",
    "print(f\"Noise ratio: {noise_ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze intra-cluster distributions and characteristics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze the distribution of the income variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_income_distribution(df, cluster_labels, income_labels):\n",
    "    df_analysis = pd.DataFrame({\n",
    "        'Cluster': cluster_labels,\n",
    "        'Income': income_labels\n",
    "    })\n",
    "    \n",
    "    # Overall cluster composition\n",
    "    print(\"Cluster Income Distribution:\")\n",
    "    for cluster in sorted(set(cluster_labels)):\n",
    "        cluster_mask = df_analysis['Cluster'] == cluster\n",
    "        cluster_size = cluster_mask.sum()\n",
    "        \n",
    "        # Get income distribution for this cluster\n",
    "        income_dist = df_analysis[cluster_mask]['Income'].value_counts(normalize=True)\n",
    "        income_counts = df_analysis[cluster_mask]['Income'].value_counts()\n",
    "        \n",
    "        cluster_name = \"Noise\" if cluster == -1 else f\"Cluster {cluster}\"\n",
    "        print(f\"\\n{cluster_name} (Size: {cluster_size})\")\n",
    "        print(f\"<=50K: {income_counts.get(0, 0)} ({income_dist.get(0, 0)*100:.1f}%)\")\n",
    "        print(f\">50K: {income_counts.get(1, 0)} ({income_dist.get(1, 0)*100:.1f}%)\")\n",
    "        \n",
    "        if cluster != -1:  # Skip noise points\n",
    "            correlation = np.corrcoef(cluster_mask, income_labels)[0,1]\n",
    "            print(f\"Correlation with income: {correlation:.3f}\")\n",
    "\n",
    "def plot_income_distribution(cluster_labels, income_labels):\n",
    "    \"\"\"\n",
    "    Visualize income distribution across clusters\n",
    "    \"\"\"\n",
    "    # Create stacked bar chart\n",
    "    df_plot = pd.DataFrame({\n",
    "        'Cluster': cluster_labels,\n",
    "        'Income': ['<=50K' if x == 0 else '>50K' for x in income_labels]\n",
    "    })\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    cluster_income = pd.crosstab(df_plot['Cluster'], df_plot['Income'], normalize='index')\n",
    "    cluster_income.plot(kind='bar', stacked=True)\n",
    "    plt.title('Income Distribution by Cluster')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.legend(title='Income')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_income_patterns(df, cluster_labels, income_labels):\n",
    "    \n",
    "    # Basic distribution analysis\n",
    "    analyze_income_distribution(df, cluster_labels, income_labels)\n",
    "    \n",
    "    # Visualization\n",
    "    plot_income_distribution(cluster_labels, income_labels)\n",
    "    \n",
    "    # Performance metrics\n",
    "analyze_income_patterns(df_transformed, cluster_labels, true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap\n",
    "\n",
    "def visualize_clusters_umap(df_transformed, cluster_labels, sample_size=48843, random_state=31): #sample size equals to actual dataset size (all datapoints plotted)\n",
    "    \"\"\"\n",
    "    Create UMAP visualization with transparent colors to show overlapping regions\n",
    "    \"\"\"\n",
    "    # Sample the data\n",
    "    if len(df_transformed) > sample_size:\n",
    "        idx = np.random.RandomState(random_state).choice(\n",
    "            len(df_transformed), sample_size, replace=False\n",
    "        )\n",
    "        df_sample = df_transformed.iloc[idx]\n",
    "        clusters_sample = cluster_labels[idx]\n",
    "    else:\n",
    "        df_sample = df_transformed\n",
    "        clusters_sample = cluster_labels\n",
    "\n",
    "    cluster_colors = {\n",
    "        -1: (1, 0, 1, 0.5),     \n",
    "        0: (0, 0, 1, 0.5),      \n",
    "        1: (1, 0, 0, 0.5),     \n",
    "        2: (0, 0.8, 0, 0.5) \n",
    "    }\n",
    "\n",
    "    # Create UMAP projection\n",
    "    print(\"Computing UMAP projection...\")\n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=15,\n",
    "        min_dist=0.1,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    embedding_umap = reducer.fit_transform(df_sample)\n",
    "\n",
    "    # Create figure\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot clusters with smaller point size and transparency\n",
    "    for cluster in cluster_labels:\n",
    "        mask = clusters_sample == cluster\n",
    "        label = 'Noise' if cluster == -1 else f'Cluster {cluster}'\n",
    "        plt.scatter(embedding_umap[mask, 0], embedding_umap[mask, 1],\n",
    "                   color=cluster_colors[cluster],\n",
    "                   s=20,  # Smaller point size\n",
    "                   label=label)\n",
    "\n",
    "    plt.xlabel('UMAP Dimension 1', fontsize=16)\n",
    "    plt.ylabel('UMAP Dimension 2', fontsize=16)\n",
    "\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    \n",
    "    # Enhanced legend\n",
    "    plt.legend(fontsize=15,              # Larger font size\n",
    "              bbox_to_anchor=(0.84, 1),  # Position legend to the right\n",
    "              loc='upper left',          # Align to upper left\n",
    "              borderaxespad=0,           # No padding\n",
    "              frameon=True,              # Add frame\n",
    "              edgecolor='black',         # Black edge color\n",
    "              fancybox=True,             # Rounded corners\n",
    "              shadow=True,\n",
    "              markerscale=4\n",
    "              )               # Add shadow\n",
    "\n",
    "    # Add grid for better readability\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()  # Adjust layout to make room for legend\n",
    "    plt.show()\n",
    "\n",
    "    # Print cluster sizes\n",
    "    unique, counts = np.unique(clusters_sample, return_counts=True)\n",
    "    print(\"\\nCluster Sizes in Sample:\")\n",
    "    for cluster, count in zip(unique, counts):\n",
    "        label = 'Noise' if cluster == -1 else f'Cluster {cluster}'\n",
    "        print(f\"{label}: {count} points ({count/len(clusters_sample):.1%})\")\n",
    "\n",
    "# Usage:\n",
    "visualize_clusters_umap(df_transformed, cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How, by changing a cost function, the demographics get affected?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First let's analyze how the demographics (in this case the gender) are distributed in the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_demographic_impact(df, cluster_labels, demographic_column):\n",
    "    \"\"\"\n",
    "    Analyze how clustering affects different demographic groups\n",
    "    \"\"\"\n",
    "    impact_analysis = pd.DataFrame({\n",
    "        'Cluster': cluster_labels,\n",
    "        'Demographic': df[demographic_column]\n",
    "    })\n",
    "    \n",
    "    # Distribution of demographics across clusters\n",
    "    cluster_demographics = pd.crosstab(\n",
    "        impact_analysis['Cluster'], \n",
    "        impact_analysis['Demographic'],\n",
    "        normalize='index'\n",
    "    )\n",
    "    \n",
    "    # Noise point analysis by demographic\n",
    "    noise_rates = impact_analysis[impact_analysis['Cluster'] == -1]['Demographic'].value_counts(normalize=True)\n",
    "    \n",
    "    return cluster_demographics, noise_rates\n",
    "\n",
    "analyze_demographic_impact(df, cluster_labels, 'gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the new model with the parameters changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HDBSCAN with different cluster size (440 instead of 220)\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=440,\n",
    "    min_samples=117,\n",
    "    cluster_selection_epsilon=0.28479667859306007,\n",
    "    metric='euclidean',\n",
    "    cluster_selection_method='eom',\n",
    "    prediction_data=True\n",
    ")\n",
    "\n",
    "# Fit and predict\n",
    "cluster_labels = clusterer.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "\n",
    "non_noise_mask = cluster_labels != -1\n",
    "sil_score = silhouette_score(X[non_noise_mask], cluster_labels[non_noise_mask])\n",
    "\n",
    "\n",
    "ari_score = adjusted_rand_score(true_labels, cluster_labels)\n",
    "nmi_score = normalized_mutual_info_score(true_labels, cluster_labels)\n",
    "noise_ratio = (cluster_labels == -1).sum() / len(cluster_labels)\n",
    "\n",
    "print(f\"Number of clusters: {n_clusters}\")\n",
    "print(f\"Silhouette score: {sil_score}\")\n",
    "print(f\"Adjusted Rand Index: {ari_score}\")\n",
    "print(f\"NMI score: {nmi_score}\")\n",
    "print(f\"Noise ratio: {noise_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_clusters, counts = np.unique(cluster_labels, return_counts=True)\n",
    "for cluster, count in zip(unique_clusters, counts):\n",
    "    cluster_name = 'Noise' if cluster == -1 else f'Cluster {cluster}'\n",
    "    print(f\"{cluster_name}: {count} points ({count/len(cluster_labels):.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_demographic_impact(df, cluster_labels, 'gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(clusterer, 'hdbscan_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'hdbscan_model.pkl'\n",
    "pickle.dump(clusterer, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
